{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import datetime\n",
    "#import json\n",
    "import logging\n",
    "import os\n",
    "import requests\n",
    "import re\n",
    "import sched\n",
    "import sys\n",
    "import time\n",
    "import timeit\n",
    "\n",
    "# Global variables \n",
    "usernames_set = set()\n",
    "counter = 0\n",
    "# Writes to data.csv in the same filepath as the script\n",
    "file = os.path.join(os.path.realpath('.'), \"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes the MAL recent users page in order to capture usernames\n",
    "def get_usernames():\n",
    "    logging.debug(\"Getting users list\")\n",
    "    url = \"https://myanimelist.net/users.php\"\n",
    "    try:\n",
    "        page = requests.get(url)\n",
    "        soup = BeautifulSoup(page.text)\n",
    "        users = soup.findAll(\"a\", {'href': re.compile(r'\\/profile\\/.*')})\n",
    "    except Exception as e:\n",
    "        logging.error(\"Error getting users list:{}\".format(str(e)))\n",
    "        return None\n",
    "    logging.debug(\"Finished getting users list\")\n",
    "    return users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function to clean the json information into a list containing tuples of the format\n",
    "(user, anime_id, status, watched_percentage, score)\n",
    "\n",
    "Status:\n",
    "• Watching - status = 1\n",
    "• Completed - status = 2\n",
    "• On Hold - status = 3\n",
    "• Dropped - status = 4\n",
    "• status = 5 seems to be unused\n",
    "• Plan to watch - status = 6\n",
    "\n",
    "watched_percentage is calculated as num_watched_episodes / anime_num_episodes\n",
    "Note: An unscored anime is calculated as a 0 by MAL\n",
    "'''\n",
    "def clean(anime_list, user):\n",
    "    logging.debug(\"Cleaning anime list for {}\".format(user.text))\n",
    "    cleaned_list = []\n",
    "    for anime in anime_list: \n",
    "        anime_id = anime.get('anime_id', None)\n",
    "        anime_status = anime.get('status', 0)\n",
    "        num_watched = anime.get('num_watched_episodes', 0)\n",
    "        num_episodes = anime.get('anime_num_episodes', 1)\n",
    "        \n",
    "        # If an anime is on MAL but has not been released yet, num_episodes will be 0\n",
    "        if num_episodes == 0:\n",
    "            watched_percentage = 0\n",
    "        else:\n",
    "            watched_percentage = num_watched / num_episodes\n",
    "        score = anime.get('score', 0)\n",
    "        cleaned_list.append((user.text, anime_id, anime_status, watched_percentage, score))\n",
    "    logging.info(\"Finished cleaning anime list for {}\".format(user.text))\n",
    "    return cleaned_list       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrapes each user's anime list\n",
    "def get_anime_lists(users):\n",
    "    global usernames_set\n",
    "    logging.debug(\"Getting anime lists\")\n",
    "    merged_anime_list = []\n",
    "    for user in users:\n",
    "        \n",
    "        # Checks to see if the user's list has already been scraped\n",
    "        if user.text in usernames_set:\n",
    "            logging.debug(\"Repeated user:{}\".format(user.text))\n",
    "            continue\n",
    "        usernames_set.add(user.text)\n",
    "        \n",
    "        # If a user's anime list is private, \n",
    "        # then trying to get the json in clean() will throw an error\n",
    "        try:\n",
    "            user_url = \"https://myanimelist.net/animelist/\" + user.text + \"/load.json?status=7&offset=0\"\n",
    "            user_page = requests.get(user_url)\n",
    "            \n",
    "            # Sleep to comply with the rate limiting\n",
    "            time.sleep(5)\n",
    "            \n",
    "            # Cleans the json information into a list\n",
    "            user_json = user_page.json()\n",
    "            anime_list = clean(user_json, user)\n",
    "            merged_anime_list.extend(anime_list)\n",
    "        except Exception as e:\n",
    "            logging.info(\"Failed getting anime list for {}:{}\".format(user.text, str(e)))\n",
    "            continue\n",
    "    logging.debug(\"Finished getting anime lists\")\n",
    "    return merged_anime_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets the cleaned anime lists and writes it to the given csv\n",
    "def scrape():\n",
    "    global file\n",
    "    logging.debug(\"Starting to scrape\")\n",
    "    start = timeit.default_timer()\n",
    "    users = get_usernames()\n",
    "    if users is not None:\n",
    "        logging.debug(\"Starting to append to csv\")\n",
    "        merged_anime_list = get_anime_lists(users)        \n",
    "        kwargs = {'newline': ''}\n",
    "        mode = 'a'\n",
    "        with open(file, mode, **kwargs) as fp:\n",
    "            writer = csv.writer(fp, delimiter=',')\n",
    "            writer.writerows(merged_anime_list)\n",
    "        logging.debug(\"Finished appending to csv\")\n",
    "    stop = timeit.default_timer()\n",
    "    logging.info(\"Runtime of scrape() was {} seconds\".format(stop-start))\n",
    "    logging.debug(\"Finished scraping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Helper function for sched\n",
    "Currently set to run 2016 times, with 1 run every 5 min\n",
    "See Data Collection.pdf for more information \n",
    "'''\n",
    "def run_task():\n",
    "    global counter\n",
    "    logging.info(\"Counter = {}\".format(counter))\n",
    "    try:\n",
    "        scrape()\n",
    "    except Exception as e:\n",
    "        logging.exception(str(e))\n",
    "    counter += 1\n",
    "    if counter >= 1512:\n",
    "        return\n",
    "    s.enter(400, 1, run_task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Logging\n",
    "timestamp = datetime.datetime.utcnow().strftime(\"%m_%d_%Y-%H_%M_%S\")\n",
    "log_filepath = os.path.join(os.path.realpath('.'), \"{}.log\".format(timestamp))\n",
    "logging.basicConfig(\n",
    "    filename = log_filepath,\n",
    "    filemode = 'w',\n",
    "    format = \"%(asctime)s-%(levelname)s-%(message)s\",\n",
    "    level = logging.INFO)\n",
    "\n",
    "\n",
    "kwargs = {'newline': ''}\n",
    "mode = 'w'\n",
    "with open(file, mode, **kwargs) as fp:\n",
    "    writer = csv.writer(fp, delimiter=',')\n",
    "    writer.writerow([\"user\", \"anime_id\", \"status\", \"watched_percentage\", \"score\"])   \n",
    "s = sched.scheduler(time.time, time.sleep)\n",
    "s.enter(60, 1, run_task)\n",
    "s.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
